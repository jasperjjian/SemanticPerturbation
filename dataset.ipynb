{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "def text_preprocess(filename):\n",
    "    f = open(filename, mode='r')\n",
    "    sent_list = f.readlines()\n",
    "    return sent_list\n",
    "\n",
    "# takes already annotated sentences and turns into a doc if necessary\n",
    "def parse_conll(filename):\n",
    "    doc = CoNLL.conll2doc(filename)\n",
    "    return doc\n",
    "\n",
    "def get_masks(sentences):\n",
    "    \"\"\"\n",
    "    [Sentence] -> {str : [str]}\n",
    "    \"\"\"\n",
    "    masked = {}\n",
    "    for s in sentences:\n",
    "        sent_dict = s.to_dict()\n",
    "        pert = []\n",
    "        for i in range(len(sent_dict)):\n",
    "            if 'upos' in sent_dict[i].keys() and sent_dict[i]['upos'] != 'PUNCT':\n",
    "                dict_copy = copy.deepcopy(sent_dict)\n",
    "                dict_copy[i]['text'] = '[MASK]'\n",
    "                pert.append((i, ''.join([w['text'] + ' ' if 'misc' not in w.keys() and w['id'] != len(sent_dict) else w['text'] for w in dict_copy])))\n",
    "        masked[s.text] = pert\n",
    "    return masked\n",
    "\n",
    "# gets the scores for each token in the masked sentence\n",
    "\n",
    "def fill_masks(masked_sentences, model_version='bert_base_uncased', number_words=20):\n",
    "    \"\"\"\n",
    "    [Sentence] -> {str : (int, str)}\n",
    "    \"\"\"\n",
    "    mask_dict = masked_sentences\n",
    "    unmasker = pipeline('fill-mask', model='bert-base-uncased', tokenizer='bert-base-uncased', device=0)\n",
    "    mask_list = [mask[1] for k in mask_dict.keys() for mask in mask_dict[k]]\n",
    "    filled_list = unmasker(mask_list, top_k=number_words)\n",
    "    filled_list = {mask_list[i] : [(word['score'], word['token_str']) for word in filled_list[i]] for i in range(len(mask_list))}\n",
    "    return filled_list\n",
    "\n",
    "def filter_by_pos(dependency_doc, filled_masks, pos_tagger):\n",
    "    mask_dict = get_masks(dependency_doc)\n",
    "    filtered_dict = {}\n",
    "    for s in dependency_doc:\n",
    "        sentence_text = s.text\n",
    "        masked_sentences = mask_dict[sentence_text]\n",
    "        for masked_sentence in masked_sentences:\n",
    "            i = masked_sentence[0]\n",
    "            masked_sentence = masked_sentence[1]\n",
    "            filtered_tokens = []\n",
    "            token_id = i\n",
    "            dict_copy = copy.deepcopy(s.to_dict())\n",
    "            pos = s.words[i].upos\n",
    "            filled_list = filled_masks[masked_sentence]\n",
    "            pert_sentences = []\n",
    "            # get a list of all the proposal perturbed sentences\n",
    "            for w in filled_list:\n",
    "                dict_copy[token_id]['text'] = w[1]\n",
    "                pert_sentence = ''.join([w['text'] + ' ' if 'misc' not in w.keys() and w['id'] != (len(s.text)) else w['text'] for w in dict_copy])\n",
    "                pert_sentences.append(pert_sentence)\n",
    "            pert_sentences_joined = '\\n\\n'.join(pert_sentences)\n",
    "            pos_doc = pos_tagger(pert_sentences_joined)\n",
    "            for j, pert_s in enumerate(pos_doc.sentences):\n",
    "                print(pert_s.text)\n",
    "                print(s.text)\n",
    "                if pert_s.words[token_id].upos == pos:\n",
    "                    filtered_tokens.append(filled_list[j][1])\n",
    "            if s.text in filtered_dict.keys():\n",
    "                filtered_dict[s.text].append((i, filtered_tokens))\n",
    "            else:\n",
    "                filtered_dict[s.text] = [(i, filtered_tokens)]\n",
    "    return filtered_dict\n",
    "\n",
    "def get_all_words(sentences):\n",
    "    all_words = {}\n",
    "    for s in sentences:\n",
    "        for w in s.words:\n",
    "            properties =  (w.upos, w.feats)\n",
    "            if properties not in all_words.keys():\n",
    "                all_words[properties] = {w.text.lower()}\n",
    "            else:\n",
    "                all_words[properties].add(w.text.lower())\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_by_template(sentences, word_dict, number_sentences=3, number_of_perturbations=2):\n",
    "    perturbed_sentences = {}\n",
    "    perturbed_categories = ['ADJ', 'ADV', 'NOUN', 'VERB', 'ADP']\n",
    "    for num_s, s in enumerate(sentences):\n",
    "        if num_s % 300 == 0:\n",
    "            print(num_s)\n",
    "        s_dict = s.to_dict()\n",
    "        pert_list = []\n",
    "        #print(s.text)\n",
    "        for iteration in range(number_sentences):\n",
    "            list_of_positions = [i for i in range(len(s.words)) if s.words[i].upos in perturbed_categories]\n",
    "            if len(list_of_positions) < number_of_perturbations:\n",
    "                continue\n",
    "            sampled_positions = random.choices(list_of_positions, k=number_of_perturbations)\n",
    "            sampled_positions = [(i, s.words[i].upos, s.words[i].feats, s.words[i].text) for i in sampled_positions]\n",
    "            dict_copy = copy.deepcopy(s.to_dict())\n",
    "            for position in sampled_positions:\n",
    "                features = (position[1], position[2])\n",
    "                possible_replacements = tuple(word_dict[features])\n",
    "                sampled_word = random.choice(possible_replacements)\n",
    "                while sampled_word == position[3].lower() and len(possible_replacements) > 1:\n",
    "                    sampled_word = random.choice(possible_replacements)\n",
    "                dict_copy[position[0]]['text'] = sampled_word\n",
    "            pert_sentence = ''.join(['' if type(w['id']) == tuple or w['upos'] == 'PUNCT' else w['text'] if i == (len(dict_copy) - 1) else w['text'] + ' ' for i, w in enumerate(dict_copy)])\n",
    "            pert_list.append(pert_sentence.strip())\n",
    "        perturbed_sentences[s.text] = pert_list\n",
    "    return perturbed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parses(sentences):\n",
    "    updated_doc = []\n",
    "    for s in sentences:\n",
    "        s_dict = s.to_dict()\n",
    "        updated_parse = [w for w in s_dict if type(w['id']) != tuple and w['upos'] != 'PUNCT']\n",
    "        updated_doc.append(updated_parse)\n",
    "    updated_doc = stanza.models.common.doc.Document(updated_doc)\n",
    "    return updated_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_updated = update_parses(d_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 2,\n",
       "  'text': 'Bush',\n",
       "  'lemma': 'Bush',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 3,\n",
       "  'deprel': 'nsubj',\n",
       "  'deps': '3:nsubj'},\n",
       " {'id': 3,\n",
       "  'text': 'fails',\n",
       "  'lemma': 'fail',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VBZ',\n",
       "  'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "  'head': 0,\n",
       "  'deprel': 'root',\n",
       "  'deps': '0:root'},\n",
       " {'id': 4,\n",
       "  'text': 'reporter',\n",
       "  'lemma': 'reporter',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NN',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 7,\n",
       "  'deprel': 'nmod:poss',\n",
       "  'deps': '7:nmod:poss'},\n",
       " {'id': 5,\n",
       "  'text': \"'s\",\n",
       "  'lemma': \"'s\",\n",
       "  'upos': 'PART',\n",
       "  'xpos': 'POS',\n",
       "  'head': 4,\n",
       "  'deprel': 'case',\n",
       "  'deps': '4:case'},\n",
       " {'id': 6,\n",
       "  'text': 'pop',\n",
       "  'lemma': 'pop',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NN',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 7,\n",
       "  'deprel': 'compound',\n",
       "  'deps': '7:compound'},\n",
       " {'id': 7,\n",
       "  'text': 'quiz',\n",
       "  'lemma': 'quiz',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NN',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 3,\n",
       "  'deprel': 'obj',\n",
       "  'deps': '3:obj'},\n",
       " {'id': 8,\n",
       "  'text': 'on',\n",
       "  'lemma': 'on',\n",
       "  'upos': 'ADP',\n",
       "  'xpos': 'IN',\n",
       "  'head': 10,\n",
       "  'deprel': 'case',\n",
       "  'deps': '10:case'},\n",
       " {'id': 9,\n",
       "  'text': 'international',\n",
       "  'lemma': 'international',\n",
       "  'upos': 'ADJ',\n",
       "  'xpos': 'JJ',\n",
       "  'feats': 'Degree=Pos',\n",
       "  'head': 10,\n",
       "  'deprel': 'amod',\n",
       "  'deps': '10:amod'},\n",
       " {'id': 10,\n",
       "  'text': 'leaders',\n",
       "  'lemma': 'leader',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NNS',\n",
       "  'feats': 'Number=Plur',\n",
       "  'head': 7,\n",
       "  'deprel': 'nmod',\n",
       "  'deps': '7:nmod:on'}]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_updated.sentences[34].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting dependency parses from doc\n",
    "def get_parses(sentences, pos_tagger, ned=False):\n",
    "    doc_all = nlp(all_sents)\n",
    "    dependency_dict = {}\n",
    "    for i,sent in enumerate(doc_all.sentences):\n",
    "        deplist = [(word.id, word.head, word.deprel) for word in sent.words]\n",
    "        deplist = [dep for dep in deplist if dep[2] != 'root']\n",
    "        dependency_dict[i] = deplist\n",
    "    target_sents_deps_labeled = {d : dependency_dict[d] for d in dependency_dict.keys() if d % 3 == 0}\n",
    "    # adds the grandparents if we want it\n",
    "    if ned:\n",
    "        for k in target_sents_deps_labeled.keys():\n",
    "            deps = target_sents_deps_labeled[k]\n",
    "            grandparents = []\n",
    "            children = [d[0] for d in deps]\n",
    "            heads = [d[1] for d in deps]\n",
    "            for d in deps:\n",
    "                head = d[1]\n",
    "                if head in children:\n",
    "                    grandparents.append((d[0], heads[children.index(head)], 'grand'))\n",
    "            target_sents_deps_labeled[k] = [*deps, *grandparents]\n",
    "    return target_sents_deps_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "300\n",
      "600\n",
      "900\n",
      "1200\n",
      "1500\n",
      "1800\n",
      "2100\n",
      "2400\n",
      "2700\n",
      "3000\n",
      "3300\n",
      "3600\n",
      "3900\n",
      "4200\n",
      "4500\n",
      "4800\n",
      "5100\n",
      "CPU times: user 13.1 s, sys: 200 ms, total: 13.3 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    #stanza.download('en')\n",
    "    #nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse', use_gpu=True, pos_batch_size=3000)\n",
    "    d = parse_conll('datasets/penntreebank-ewt.conllu')\n",
    "    d_prime = [s for s in d.sentences if len(s.words) <= 15 and len(s.words) >= 4]\n",
    "    #masked_sentences = get_masks(d_prime)\n",
    "    #filled_masks = fill_masks(masked_sentences)\n",
    "    \"\"\"\n",
    "    with open('test_fill_replacement.pkl', 'wb') as f:\n",
    "        pickle.dump(test_fill, f)\n",
    "    \"\"\"\n",
    "    labeled_words = get_all_words(d.sentences)\n",
    "    test_fill = fill_by_template(d_prime, labeled_words, number_sentences=9, number_of_perturbations=2)\n",
    "    with open('test_fill_replacement.pkl', 'wb') as f:\n",
    "        pickle.dump(test_fill, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "CHERNOBYL ACCIDENT: TEN YEARS ON\n",
      "ACCIDENT\n",
      "punct\n",
      "Here are some excerpts:\n",
      "Here\n",
      "punct\n",
      "Let's just remember a seminal Bush moment in 1999:\n",
      "Let's\n",
      "punct\n",
      "Web posted at: 3:29 p.m. EST (2029 GMT)\n",
      "p.m.\n",
      "punct\n",
      "you can check it out : los angeles online dating\n",
      "dating\n",
      "punct\n",
      "WADE GOODWYN reporting:\n",
      "reporting\n",
      "punct\n",
      "Ms. BABA GROOM (Former Campaign Worker):\n",
      "Ms.\n",
      "punct\n",
      "Mr. ARCHIBALD: People have different ways of starting the days in any office.\n",
      "Mr.\n",
      "punct\n",
      "(also see this thread on the RI discussion board):\n",
      "see\n",
      "punct\n",
      "And it's guilty :\n",
      "'s\n",
      "punct\n",
      "From SNAP's statement on the Robinson conviction:\n",
      "'s\n",
      "punct\n",
      "The murder weapon, Robinson's letter opener:\n",
      "weapon\n",
      "punct\n",
      "SPLOID.com cited you on the topic of that priest conviction: http://www.sploid.com/news/2006/05/evil_priest_gui.php\n",
      "http://www.sploid.com/news/2006/05/evil_priest_gui.php\n",
      "punct\n",
      "The following quote is from that Coogan article:\n",
      "Coogan\n",
      "punct\n",
      "Here's another interesting example:\n",
      "Here's\n",
      "punct\n",
      "Not 200,000 guns - the numbers dont work:\n",
      "guns\n",
      "punct\n",
      "Important Kos Diary about bush's response to the gathering storm of scandals:\n",
      "Diary\n",
      "punct\n",
      "In scanning the whitehouse.gov site today, I noticed the following Executive Order:\n",
      "noticed\n",
      "punct\n",
      "Tronicus, if you're still checking this thread:\n",
      "still\n",
      "punct\n",
      "Al Qaeda, Anthrax and Ayman: Means, Motive, Modus Operandi and Opportunity\n",
      "Qaeda\n",
      "punct\n",
      "Current Salary: $47,500\n",
      "Salary\n",
      "punct\n",
      "Job Group: Specialist\n",
      "Group\n",
      "punct\n",
      "YE PRC Rating: Satisfactory\n",
      "Rating\n",
      "punct\n",
      "Base Salary: 55k\n",
      "Salary\n",
      "punct\n",
      "Salary Listing in Job Group:\n",
      "Listing\n",
      "punct\n",
      "WHAT: Happy Hour for John Suarez\n",
      "WHAT\n",
      "punct\n",
      "WHEN: Today at 5pm\n",
      "Today\n",
      "punct\n",
      "WHERE: The Front Porch 217 Gray St. (713) 571-9571\n",
      "Porch\n",
      "punct\n",
      "WHY: Today is John's last day at EBS.\n",
      "WHY\n",
      "punct\n",
      "I have a couple of questions so I can wrap up the LOI:\n",
      "have\n",
      "punct\n",
      "I have a couple of questions so I can wrap up the LOI:\n",
      "have\n",
      "punct\n",
      "Sent by: Ben Jacoby@ECT\n",
      "Ben\n",
      "punct\n",
      "Sent by: Jeff Dasovich\n",
      "Jeff\n",
      "punct\n",
      "Sent by: Jeff Dasovich\n",
      "Jeff\n",
      "punct\n",
      "There are still four important issues, however:\n",
      "are\n",
      "punct\n",
      "Sent by: Lysa Akin@ECT\n",
      "Lysa\n",
      "punct\n",
      "Please be advised that I have set the call as follows:\n",
      "advised\n",
      "punct\n",
      "Date: Monday, Nov. 13th\n",
      "Date\n",
      "punct\n",
      "Time: 11:30am / 1:30pm Central / 2:30pm Eastern\n",
      "Time\n",
      "punct\n",
      "Call In #: 888-422-7132\n",
      "#\n",
      "punct\n",
      "Pin #: 411507\n",
      "#\n",
      "punct\n",
      "Pin # for Paul Kaufman ONLY: 362416\n",
      "#\n",
      "punct\n",
      "- ENRON.XLS << File: ENRON.XLS >>\n",
      "File\n",
      "punct\n",
      "- enrongss.xls << File: enrongss.xls >>\n",
      "File\n",
      "punct\n",
      "- ENRON.XLS << File: ENRON.XLS >>\n",
      "File\n",
      "punct\n",
      "- enrongss.xls << File: enrongss.xls >>\n",
      "File\n",
      "punct\n",
      "- ENRON.XLS << File: ENRON.XLS >>\n",
      "File\n",
      "punct\n",
      "- enrongss.xls << File: enrongss.xls >>\n",
      "File\n",
      "punct\n",
      "Please find enclosed EES's request for Volumes for PGE CityGate delvery effective 11/1/01:\n",
      "find\n",
      "punct\n",
      "Cindy Franklin Transportation Services Work:832.676.3177 Fax: 832.676.1329 Pager: 1.888.509.3736\n",
      "Work\n",
      "punct\n",
      "Cindy Franklin Transportation Services Work:832.676.3177 Fax: 832.676.1329 Pager: 1.888.509.3736\n",
      "Fax\n",
      "punct\n",
      "Cindy Franklin Transportation Services Work:832.676.3177 Fax: 832.676.1329 Pager: 1.888.509.3736\n",
      "Pager\n",
      "punct\n",
      "Conference Plus will be hosting this call:\n",
      "hosting\n",
      "punct\n",
      "the date: Thursday, March 22nd, 2001\n",
      "date\n",
      "punct\n",
      "the number: 1-800-991-9019\n",
      "number\n",
      "punct\n",
      "the passcode: 6871082#\n",
      "passcode\n",
      "punct\n",
      "the time: 10:00 AM - 11:00 AM CST\n",
      "time\n",
      "punct\n",
      "the place: EB 3143C\n",
      "place\n",
      "punct\n",
      "the subject: Turbine 1 and Turbine 2 Purchase Agreement\n",
      "subject\n",
      "punct\n",
      "Conference Plus will be hosting this call:\n",
      "hosting\n",
      "punct\n",
      "the date: Thursday, March 22nd, 2001\n",
      "date\n",
      "punct\n",
      "the number: 1-800-991-9019\n",
      "number\n",
      "punct\n",
      "the passcode: 6871082#\n",
      "passcode\n",
      "punct\n",
      "the time: 10:00 AM - 11:00 AM CST\n",
      "time\n",
      "punct\n",
      "the place: EB 3143C\n",
      "place\n",
      "punct\n",
      "the subject: Turbine 1 and Turbine 2 Purchase Agreement\n",
      "subject\n",
      "punct\n",
      "Here are the dates:\n",
      "Here\n",
      "punct\n",
      "Could you please let us know which of these would be possible for you:\n",
      "let\n",
      "punct\n",
      "NOTE: The information in this email is confidential and may be legally privileged.\n",
      "NOTE\n",
      "punct\n",
      "Sent by: Ernie Simien\n",
      "Ernie\n",
      "punct\n",
      "Subject: Re: IF TGPL LA Z1\n",
      "Subject\n",
      "punct\n",
      "Subject: Re: IF TGPL LA Z1\n",
      "Z1\n",
      "punct\n",
      "<< File: Ineos.xls >>\n",
      "File\n",
      "punct\n",
      "Hi, Liz:\n",
      "Hi\n",
      "punct\n",
      "<< File: Tabors Conflict Letter Alberta Export 050901.doc >>\n",
      "File\n",
      "punct\n",
      "e.g.: EnronEAuction.\n",
      "EnronEAuction\n",
      "punct\n",
      "Sent by: Nella Cappelletto\n",
      "Nella\n",
      "punct\n",
      "(See attached file: ETA_revision0307.doc)\n",
      "file\n",
      "punct\n",
      "Confirmation Desk: you can stop adding the rep to the confirms now.\n",
      "stop\n",
      "punct\n",
      "As such, please provide the following detail:\n",
      "provide\n",
      "punct\n",
      "- BRENNER, URSULA J.doc << File: BRENNER, URSULA J.doc >>\n",
      "File\n",
      "punct\n",
      "Preseason Game A: Saturday, May 12 @ 7:30PM vs. Miami SOL\n",
      "A\n",
      "punct\n",
      "Preseason Game B: Thursday, May 24 @ 7:30PM vs. Detroit SHOCK\n",
      "B\n",
      "punct\n",
      "Game 1: Monday, May 28 @ 2:00PM vs. Los Angeles SPARKS\n",
      "Game\n",
      "punct\n",
      "Game 3: Monday, June 11 @ 7:00PM vs. Los Angeles SPARKS\n",
      "Game\n",
      "punct\n",
      "Game 4: Thursday, June 14 @ 7:30PM vs. Portland FIRE\n",
      "Game\n",
      "punct\n",
      "Game 5: Sunday, June 17 @ 1:00PM vs. Utah STARZZ\n",
      "Game\n",
      "punct\n",
      "Game 6: Tuesday, June 19 @ 7:30PM vs. Washington MYSTICS\n",
      "Game\n",
      "punct\n",
      "Game 7: Saturday, June 23 @ 3:00PM vs. Sacramento MONARCHS\n",
      "Game\n",
      "punct\n",
      "Game 8: Monday, July 2 @ 7:30PM vs. Portland FIRE\n",
      "Game\n",
      "punct\n",
      "Game 9: Friday, July 6 @ 7:30PM vs. Indiana FEVER\n",
      "Game\n",
      "punct\n",
      "Game 10: Sunday, July 8 @ 1:00PM vs. Cleveland ROCKERS\n",
      "Game\n",
      "punct\n",
      "Game 11: Tuesday, July 24 @ 7:30PM vs. Utah STARZZ\n",
      "Game\n",
      "punct\n",
      "Game 12: Saturday, July 28 @ 12:30PM vs. New York LIBERTY\n",
      "Game\n",
      "punct\n",
      "Game 13: Monday, July 30 @ 7:30PM vs. Seattle STORM\n",
      "Game\n",
      "punct\n",
      "Game 14: Friday, August 3 @ 7:30PM vs. Orlando MIRACLE\n",
      "Game\n",
      "punct\n",
      "Game 15: Monday, August 6 @ 7:00PM vs. Phoenix MERCURY\n",
      "Game\n",
      "punct\n",
      "Game 16: Monday, August 13 @ 7:30PM vs. Minnesota LYNX\n",
      "Game\n",
      "punct\n",
      "Date: Monday, May 7th\n",
      "Date\n",
      "punct\n",
      "Time: 11:00 a.m. (CDT)\n",
      "Time\n",
      "punct\n",
      "Location: 50th Floor Boardroom\n",
      "Location\n",
      "punct\n",
      "Video: Connections will be established with remote locations upon request.\n",
      "Video\n",
      "punct\n",
      "Conf call: AT&T lines have been reserved.\n",
      "call\n",
      "punct\n",
      "Please review the process below:\n",
      "review\n",
      "punct\n",
      "Re: Cargill Ferrous International\n",
      "International\n",
      "punct\n",
      "Website Short Description of Product Type: US HR StlPlt Phy\n",
      "Description\n",
      "punct\n",
      "Product Additional Information (example):\n",
      "Information\n",
      "punct\n",
      "Unit of Measure:\n",
      "Unit\n",
      "punct\n",
      "STEPS FOR APPROVAL:\n",
      "STEPS\n",
      "punct\n",
      "TO APPROVE: Right mouse click on \"Approved\"\n",
      "click\n",
      "punct\n",
      "Please review the process below:\n",
      "review\n",
      "punct\n",
      "Re: Cargill Ferrous International\n",
      "International\n",
      "punct\n",
      "Where: Conf. Call\n",
      "Call\n",
      "punct\n",
      "Call-in #: 800/711-8000\n",
      "#\n",
      "punct\n",
      "<< OLE Object: Picture (Device Independent Bitmap) >>\n",
      "Object\n",
      "punct\n",
      "To Whom It May Concern:\n",
      "Whom\n",
      "punct\n",
      "A reminder that the HR Associate Points Meeting will take place as below:\n",
      "reminder\n",
      "punct\n",
      "The Leads who are responsible for Associates are as follows:\n",
      "are\n",
      "punct\n",
      "FYI re: NEPCO picketing issues.\n",
      "issues\n",
      "punct\n",
      "NOTICE: This message is confidential.\n",
      "NOTICE\n",
      "punct\n",
      "Dan: please see if the attached draft works.\n",
      "see\n",
      "punct\n",
      "Gas Accord II Settlement Participants:\n",
      "Participants\n",
      "punct\n",
      "Greetings Judge Minkin:\n",
      "Greetings\n",
      "punct\n",
      "Sent by: Jeff Dasovich\n",
      "Jeff\n",
      "punct\n",
      "Campus interviews are scheduled as follows:\n",
      "scheduled\n",
      "punct\n",
      "Place: University of Pennsylvania\n",
      "Place\n",
      "punct\n",
      "Date: Thursday, March 1st Friday, March 2nd\n",
      "Date\n",
      "punct\n",
      "Where: On Campus Inn at Pen\n",
      "Where\n",
      "punct\n",
      "Time: 8:00 - 5:00 8:00 - 5:00\n",
      "Time\n",
      "punct\n",
      "Interviewers: Kevin McGowan- confirmed\n",
      "Interviewers\n",
      "punct\n",
      "Marianne: The following are my comments:\n",
      "comments\n",
      "punct\n",
      "Marianne: The following are my comments:\n",
      "comments\n",
      "punct\n",
      "Please note the following as you review:\n",
      "note\n",
      "punct\n",
      "Christie or Cass:\n",
      "Christie\n",
      "punct\n",
      "<< File: Questar So Trails Hub Interconnect 02-05-02.doc >>\n",
      "File\n",
      "punct\n",
      "<< File: Questar So Trails Hub Interconnect 02-05-02.doc >>\n",
      "File\n",
      "punct\n",
      "Thanks: John Buchanan\n",
      "Thanks\n",
      "punct\n",
      "Thanks: John Buchanan\n",
      "Thanks\n",
      "punct\n",
      "This difference is mainly due to the following:\n",
      "the\n",
      "punct\n",
      "(See attached file: Sanders Letter 4_28_00.doc)\n",
      "file\n",
      "punct\n",
      "WHEN: 11am - noon\n",
      "WHEN\n",
      "punct\n",
      "WHAT: Copies of the filed complaint will be made available at that time.\n",
      "WHAT\n",
      "punct\n",
      "But supporters are pointing a few good things out:\n",
      "pointing\n",
      "punct\n",
      "Consider these convenience factors:\n",
      "Consider\n",
      "punct\n",
      "Here are the numbers to call:\n",
      "Here\n",
      "punct\n",
      "The Passcode to participate is:5107\n",
      "5107\n",
      "punct\n",
      "DATE: Tuesday, November 22, 2005\n",
      "DATE\n",
      "punct\n",
      "TIME: 8:00 PM Eastern Standard Time\n",
      "TIME\n",
      "punct\n",
      "LOCATION: Conference Call\n",
      "LOCATION\n",
      "punct\n",
      "Email: \"Dharmadeva\"<dharmad...@gmail.com>\n",
      "Email\n",
      "punct\n",
      "Bending forward suggests: \"I completely surrender to You.\"\n",
      "suggests\n",
      "punct\n",
      "Email : mayur...@yahoo.com SMS : +919819602175 Web :\n",
      "Email\n",
      "punct\n",
      "Email : mayur...@yahoo.com SMS : +919819602175 Web :\n",
      "SMS\n",
      "punct\n",
      "Email : mayur...@yahoo.com SMS : +919819602175 Web :\n",
      "Web\n",
      "punct\n",
      "Subject: What is \"food for the NSA line-eater\"?\n",
      "Subject\n",
      "punct\n",
      "Email: \"Peanutjake\"<peanutjak...@usa.com>\n",
      "Email\n",
      "punct\n",
      "Subject: IMMEDIATE ATTENTION NEEDED:\n",
      "Subject\n",
      "punct\n",
      "Subject: IMMEDIATE ATTENTION NEEDED:\n",
      "Subject\n",
      "punct\n",
      "FROM: GEORGE WALKER BUSH 202.456.1414 / 202.456.1111 FAX: 202.456.2461\n",
      "GEORGE\n",
      "punct\n",
      "FROM: GEORGE WALKER BUSH 202.456.1414 / 202.456.1111 FAX: 202.456.2461\n",
      "FAX\n",
      "punct\n",
      "George Walker Bush Switchboard: 202.456.1414 Comments: 202.456.1111 Fax: 202.456.2461 e-mail: presid...@whitehouse.gov\n",
      "Switchboard\n",
      "punct\n",
      "George Walker Bush Switchboard: 202.456.1414 Comments: 202.456.1111 Fax: 202.456.2461 e-mail: presid...@whitehouse.gov\n",
      "Comments\n",
      "punct\n",
      "George Walker Bush Switchboard: 202.456.1414 Comments: 202.456.1111 Fax: 202.456.2461 e-mail: presid...@whitehouse.gov\n",
      "Fax\n",
      "punct\n",
      "George Walker Bush Switchboard: 202.456.1414 Comments: 202.456.1111 Fax: 202.456.2461 e-mail: presid...@whitehouse.gov\n",
      "e-mail\n",
      "punct\n",
      "MOON LANDING HOAX: CHURCH = TECHNOLOGY & GOVERNMENT - Shuttle Carried on Aircraft model\n",
      "HOAX\n",
      "punct\n",
      "Q: NASA IS A WASTE!\n",
      "Q\n",
      "punct\n",
      "Subject: NASA IS A WASTE!\n",
      "Subject\n",
      "punct\n",
      "Category: Science > Astronomy\n",
      "Category\n",
      "punct\n",
      "Asked by: yheggy-ga\n",
      "ga\n",
      "punct\n",
      "List Price: $2.00\n",
      "Price\n",
      "punct\n",
      "Posted: 25 Oct 2004 18:32 PDT\n",
      "Posted\n",
      "punct\n",
      "Expires: 24 Nov 2004 17:32 PST\n",
      "Expires\n",
      "punct\n",
      "Question ID: 420072\n",
      "ID\n",
      "punct\n",
      "Subject: Re: NASA IS A WASTE!\n",
      "Subject\n",
      "punct\n",
      "Subject: Re: NASA IS A WASTE!\n",
      "WASTE\n",
      "punct\n",
      "Answered By: googlenut-ga on 25 Oct 2004 19:14 PDT\n",
      "ga\n",
      "punct\n",
      "-- The Moon/Mars vision:\n",
      "vision\n",
      "punct\n",
      "Google Search Terms:\n",
      "Terms\n",
      "punct\n",
      "Subject: Re: NASA IS A WASTE!\n",
      "Subject\n",
      "punct\n",
      "Subject: Re: NASA IS A WASTE!\n",
      "WASTE\n",
      "punct\n",
      "From: saem_aero-ga on 26 Oct 2004 18:11 PDT\n",
      "ga\n",
      "punct\n",
      "WHEN: JUNE 12, 2005, 2:00 P.M. TO 2:00 A.M.\n",
      "JUNE\n",
      "punct\n",
      "WHERE: MAIN STREET SALOON\n",
      "SALOON\n",
      "punct\n",
      "ADMISSION: DONATIONS AT THE DOOR\n",
      "ADMISSION\n",
      "punct\n",
      "(Filed: 19/11/2004)\n",
      "Filed\n",
      "punct\n",
      "Now start to repeat within your mind the mantra: BABA NAM KEVALAM.\n",
      "mantra\n",
      "punct\n",
      "Email: Bill McGinnis <bmc...@patriot.net>\n",
      "Email\n",
      "punct\n",
      "My Fellow Americans:\n",
      "Americans\n",
      "punct\n",
      "For details and further proof, please see these websites:\n",
      "see\n",
      "punct\n",
      "Email: \"righter\"<righ...@sonic.net>\n",
      "Email\n",
      "punct\n",
      "Here's the text of the email:\n",
      "Here's\n",
      "punct\n",
      "From: Travis Job <t...@sonic.net>\n",
      "Job\n",
      "punct\n",
      "Date: Thu, 11 Mar 2004 02:39:27 -0800\n",
      "Date\n",
      "punct\n",
      "To: SPAR <s...@sonic.net>\n",
      "SPAR\n",
      "punct\n",
      "Subject: Civet Cats, James Rachels + a song\n",
      "Subject\n",
      "punct\n",
      "But this is what he says about civet cats:\n",
      "what\n",
      "punct\n",
      "icq uin: 5249025\n",
      "uin\n",
      "punct\n",
      "\"Note to Wigner: Do you think your friend could feed my cat?\n",
      "Note\n",
      "punct\n",
      "Suggestion: Read this entire message carefully!!\n",
      "Suggestion\n",
      "punct\n",
      "Mail the six envelopes to the following addresses:\n",
      "Mail\n",
      "punct\n",
      "Now post your amended article to at least 200 news groups. :\n",
      "post\n",
      "punct\n",
      "Blue Planet: Who's afraid of Big Bad Wolf?\n",
      "Planet\n",
      "punct\n",
      "They seem to be just that: rumors.\n",
      "that\n",
      "punct\n",
      "Climate: Humans fuss, animals adjust\n",
      "Climate\n",
      "punct\n",
      "Some of the better comments:\n",
      "Some\n",
      "punct\n",
      "ARCHILOCHUS solar eclipse: April 6, 648 BC Friday\n",
      "eclipse\n",
      "punct\n",
      "Koran descends to Earth: April 6, 610 AD Monday\n",
      "April\n",
      "punct\n",
      "CLEMENT's St.Methodius dies: April 6, 884 Monday\n",
      ":\n",
      "punct\n",
      "Petrarch meets LAURA: April 6, 1327 Monday\n",
      "April\n",
      "punct\n",
      "DURER dies: April 6, 1528 Monday\n",
      "April\n",
      "punct\n",
      "BRIDGET Vere's birth: April 6, 1584 Monday\n",
      "'s\n",
      "punct\n",
      "Sir Francis Walsingham dies: April 6, 1590 Monday\n",
      "April\n",
      "punct\n",
      "\"native of Crete\" EL GRECO dies: April 7, 1614 Monday\n",
      "April\n",
      "punct\n",
      "{LUCIO: Does BRIDGET PAINT still, Pompey, ha?\n",
      "LUCIO\n",
      "punct\n",
      "LAURA dies of plague: April 6, 1348 Sunday\n",
      "April\n",
      "punct\n",
      "RAPHAEL born: April 6, 1483 Sunday\n",
      "April\n",
      "punct\n",
      "RAPHAEL dies: April 6, 1520 Good Friday\n",
      "April\n",
      "punct\n",
      "Thomas Hobbes' birth: April 5, 1588 Good Friday\n",
      "'\n",
      "punct\n",
      "Kent EARTHQUAKE: April 6, 1580 Wednesday\n",
      "EARTHQUAKE\n",
      "punct\n",
      "Historian John Stow dies: April 6, 1605 Sat/Wed.\n",
      "April\n",
      "punct\n",
      "From: Martin Green <MGr...@usa.pipeline.com>\n",
      "Martin\n",
      "punct\n",
      "Date: Sunday, 22 Oct 1995 10:57:32 -0400\n",
      "Date\n",
      "punct\n",
      "Subject: Re: Facts, Purpose of List, Italy, Jews\n",
      "Subject\n",
      "punct\n",
      "Subject: Re: Facts, Purpose of List, Italy, Jews\n",
      "Facts\n",
      "punct\n",
      "Email: yamwhatiyam <pop...@spinach.eat>\n",
      "Email\n",
      "punct\n",
      "Groups: alt.consumers, ba.consumers, misc.consumers, misc.consumers.frugal-living\n",
      "Groups\n",
      "punct\n",
      "Attack on Iran: A Looming Folly\n",
      "Attack\n",
      "punct\n",
      "Conclusion: Is Any of This Possible?\n",
      "Conclusion\n",
      "punct\n",
      "The answer lies in one now-familiar name: Jack Abramoff.\n",
      "name\n",
      "punct\n",
      "George Bush: Military man\n",
      "George\n",
      "punct\n",
      "George Bush: Money manager\n",
      "George\n",
      "punct\n",
      "George Bush: Tax cutter\n",
      "George\n",
      "punct\n",
      "George Bush: Lawman\n",
      "George\n",
      "punct\n",
      "The key is...I have a few problems:\n",
      "is\n",
      "punct\n",
      "EDIT: now i'm curious...\n",
      "EDIT\n",
      "punct\n",
      "EDIT: How did I know...\n",
      "EDIT\n",
      "punct\n",
      "Our Price: $10.99 to 12.99 PETSMART!\n",
      "Price\n",
      "punct\n",
      "This is the cage I have whhich is pretty much IMPOSSIBLE to escape from:\n",
      "cage\n",
      "punct\n",
      "Many online stores carry them, like Adorama:\n",
      "carry\n",
      "punct\n",
      "For more info on lo-fi photography, check put my website:\n",
      "check\n",
      "punct\n",
      "Lotte World: http://3.bp.blogspot.com/-X_e2uwT6wPw/Tkj_7UVTw6I/AAAAAAAAAGs/e_hICAdYPYI/s1600/lotte_world_from_high_up.jpg\n",
      "World\n",
      "punct\n",
      "Everland Resort: http://v2.cache7.c.bigcache.googleapis.com/static.panoramio.com/photos/original/42661265.jpg?redirect_counter=2\n",
      "Resort\n",
      "punct\n",
      "Carribbean Bay: http://tong.visitkorea.or.kr/cms/resource/81/188181_image2_1.jpg\n",
      "Bay\n",
      "punct\n",
      "Namsan tower: http://farm3.static.flickr.com/2406/2527255596_db23df940f.jpg\n",
      "tower\n",
      "punct\n",
      "On the RER/Metro:\n",
      "RER\n",
      "punct\n",
      "On the Air France bus:\n",
      "bus\n",
      "punct\n",
      "On the Metro: Get in the Metro at Opera.\n",
      "Get\n",
      "punct\n",
      "Here are some better options IMO:\n",
      "Here\n",
      "punct\n",
      "Directions from the Kennedy: Take the Ogden Exit off the Kennedy.\n",
      "Directions\n",
      "punct\n",
      "Hours of Operation:\n",
      "Hours\n",
      "punct\n",
      "Please remember: there is NO such thing.\n",
      "remember\n",
      "punct\n",
      "Skin Color: Silkies have black skin color, which is dominate.\n",
      "Color\n",
      "punct\n",
      "Five Toes: Silkies have five toes.\n",
      "Toes\n",
      "punct\n",
      "My general convince your parents to let you get a reptile advice:\n",
      "advice\n",
      "punct\n",
      "include which one had:\n",
      "include\n",
      "punct\n",
      "To answer your questions in order:\n",
      "answer\n",
      "punct\n",
      "True or false: being a young New Zealander means having no future?\n",
      "True\n",
      "punct\n",
      "If a pair isn't leaving eggs... check the following:\n",
      "...\n",
      "punct\n",
      "Here's what the owner has said:\n",
      "Here's\n",
      "punct\n",
      "This place is the opposite of QuikTrip: crappy in every way.\n",
      "crappy\n",
      "punct\n",
      "I tell them: why not just go with these guys?\n",
      "tell\n",
      "punct\n",
      "Other Thoughts: Will try this place again.\n",
      "Thoughts\n",
      "punct\n",
      "Cranmore Dental and Implant Clinic : I could not recommend Dr David Nelson enough.\n",
      "Clinic\n",
      "punct\n",
      "Andrew was helpful and knowledgeable about acupuncture re: infertility.\n",
      "infertility\n",
      "punct\n",
      "Poor Service, Lack of Passion: Do NOT go\n",
      "Service\n",
      "punct\n",
      "anyways, the mezza luna: you should try it.\n",
      "luna\n",
      "punct\n",
      "Update: I had to add to my review.\n",
      "Update\n",
      "punct\n",
      "And he says: You're at Warwick in Pennsylvania?\n",
      "says\n",
      "punct\n",
      "he says: Why you tell me your in WARWICK TOWNSHIP?\n",
      "says\n",
      "punct\n",
      "Seriously: do not waste your time.\n",
      "Seriously\n"
     ]
    }
   ],
   "source": [
    "for s in d_prime:\n",
    "    punct_list = []\n",
    "    for w in s.words:\n",
    "        if w.text == ':':\n",
    "            punct_list.append(w.id)\n",
    "            print(w.deprel)\n",
    "            print(s.text)\n",
    "            print(list(s.to_dict())[w.head - 1]['text'])\n",
    "    for w in s.words:\n",
    "        if w.head in punct_list:\n",
    "            print(s.text)\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (default-env)",
   "language": "python",
   "name": "default-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
